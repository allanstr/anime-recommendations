{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for anime scrapping from MyAnimeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anime:\n",
    "  def __init__(self, title, href):\n",
    "    self.Title = title\n",
    "    self.href = href\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(href, headers=headers)\n",
    "    self.html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  def scrape_anime_id(self):\n",
    "    self.Id = int(self.href.split('https://myanimelist.net/anime/')[1].split('/')[0])\n",
    "\n",
    "  def scrape_english_title(self):\n",
    "    try:\n",
    "      english_title = float(\n",
    "          self.html\n",
    "          .find(class_='title-english title-inherit')\n",
    "          .get_text(strip=True)\n",
    "      )\n",
    "    except:\n",
    "      english_title = np.nan\n",
    "    self.English_title = english_title\n",
    "    return english_title\n",
    "\n",
    "  def scrape_score(self):\n",
    "    try:\n",
    "      score = float(\n",
    "          self.html\n",
    "          .find(class_='fl-l score')\n",
    "          .get_text(strip=True)\n",
    "      )\n",
    "    except:\n",
    "      score = np.nan\n",
    "    self.Score = score\n",
    "    return score\n",
    "\n",
    "  def scrape_users_scoring(self):\n",
    "    try:\n",
    "      users_scoring = int(\n",
    "          self.html\n",
    "          .find(class_='fl-l score')\n",
    "          .get('data-user')\n",
    "          .replace(',', '')\n",
    "          .replace(' users', '')\n",
    "          .replace('-', '0')\n",
    "      )\n",
    "    except:\n",
    "      users_scoring = np.nan\n",
    "    self.Users_scoring = users_scoring\n",
    "    return users_scoring\n",
    "\n",
    "  def scrape_rank(self):\n",
    "    try:\n",
    "      rank = int(\n",
    "          self.html\n",
    "          .find(class_='numbers ranked')\n",
    "          .get_text(strip=True)\n",
    "          .replace('Ranked#', '')\n",
    "      )\n",
    "    except:\n",
    "      rank = np.nan\n",
    "    self.Rank = rank\n",
    "    return rank\n",
    "\n",
    "  def scrape_popularity(self):\n",
    "    try:\n",
    "      popularity = int(\n",
    "          self.html\n",
    "          .find(class_='numbers popularity')\n",
    "          .get_text(strip=True)\n",
    "          .replace('Popularity#', '')\n",
    "      )\n",
    "    except:\n",
    "      popularity = np.nan\n",
    "    self.Popularity = popularity\n",
    "    return popularity\n",
    "\n",
    "  def scrape_members(self):\n",
    "    try:\n",
    "      members = int(\n",
    "          self.html\n",
    "          .find(class_='numbers members')\n",
    "          .get_text(strip=True)\n",
    "          .replace('Members', '')\n",
    "          .replace(',', '')\n",
    "      )\n",
    "    except:\n",
    "      members = np.nan\n",
    "    self.Members = members\n",
    "    return members\n",
    "\n",
    "  def scrape_season(self):\n",
    "    try:\n",
    "      season = (\n",
    "          self.html\n",
    "          .find(class_='information season')\n",
    "          .get_text(strip=True)\n",
    "      )\n",
    "    except:\n",
    "      season = np.nan\n",
    "    self.Season = season\n",
    "    return season\n",
    "\n",
    "  def scrape_show_type(self):\n",
    "    try:\n",
    "      show_type = (\n",
    "          self.html\n",
    "          .find(class_='information type')\n",
    "          .get_text(strip=True)\n",
    "      )\n",
    "    except:\n",
    "      show_type = np.nan\n",
    "    self.Show_type = show_type\n",
    "    return show_type\n",
    "\n",
    "  def scrape_studio(self):\n",
    "    try:\n",
    "      studio = (\n",
    "          self.html\n",
    "          .find(class_='information studio author')\n",
    "          .get_text(strip=True)\n",
    "      )\n",
    "    except:\n",
    "      studio = np.nan\n",
    "    self.Studio = studio\n",
    "    return studio\n",
    "\n",
    "  def scrape_synopsis(self):\n",
    "    try:\n",
    "      synopsis = (\n",
    "          self.html\n",
    "          .find(itemprop='description')\n",
    "          .get_text(separator=' ', strip=True)\n",
    "      )\n",
    "    except:\n",
    "      synopsis = np.nan\n",
    "    self.Synopsis = synopsis\n",
    "    return synopsis\n",
    "\n",
    "  def scrape_infos(self):\n",
    "    infos = self.html.find_all(class_='spaceit_pad')\n",
    "\n",
    "    matches = ['English:', 'Studios:', 'Episodes:', 'Status:', 'Aired:', 'Duration:', 'Broadcast:', 'Source:', 'Rating:', 'Demographic:']\n",
    "    infos_dict = {}\n",
    "\n",
    "    for info in infos:\n",
    "      txt = info.get_text('\\n', strip=True)\n",
    "      if any(m in txt for m in matches):\n",
    "        register = txt.replace(':', '').splitlines()\n",
    "        infos_dict[register[0]] = register[1]\n",
    "\n",
    "      if info.find(string=re.compile('Genre:|Genres:')) != None:\n",
    "        genre = [i.get_text(strip=True) for i in info.find_all('a')]\n",
    "        infos_dict['Genre'] = genre#';'.join(genre)\n",
    "\n",
    "      if info.find(string=re.compile('Producers:')) != None:\n",
    "        producer = [i.get_text(strip=True) for i in info.find_all('a')]\n",
    "        infos_dict['Producers'] = producer#';'.join(producer)\n",
    "\n",
    "      if info.find(string=re.compile('Theme:|Themes:')) != None:\n",
    "        infos_dict['Theme'] = info.find('a').get_text(strip=True)\n",
    "\n",
    "      for k, v in infos_dict.items():\n",
    "        setattr(self, k, v)\n",
    "\n",
    "    return infos_dict\n",
    "\n",
    "  def scrape_all(self):\n",
    "    self.scrape_anime_id()\n",
    "    #anime.scrape_english_title()\n",
    "    self.scrape_score()\n",
    "    self.scrape_users_scoring()\n",
    "    self.scrape_rank()\n",
    "    self.scrape_popularity()\n",
    "    self.scrape_members()\n",
    "    self.scrape_season()\n",
    "    self.scrape_show_type()\n",
    "    #anime.scrape_studio()\n",
    "    self.scrape_synopsis()\n",
    "    self.scrape_infos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_animes_by_season(year, season):\n",
    "  site = f'https://myanimelist.net/anime/season/{str(year)}/{season}'\n",
    "\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}\n",
    "  response = requests.get(site, headers=headers)\n",
    "\n",
    "  html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  animes = html.find_all(class_='h2_anime_title')\n",
    "  titles = [anime.get_text() for anime in animes]\n",
    "  hrefs = [anime.find('a').get('href') for anime in animes]\n",
    "\n",
    "  return titles, hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_anime(title, href):\n",
    "    try:\n",
    "        anime = Anime(title=title, href=href)\n",
    "        anime.scrape_all()\n",
    "        data = vars(anime)\n",
    "        data.pop('html', None)\n",
    "    except:\n",
    "        data = np.nan\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_animes_by_season(years, seasons):\n",
    "    for year in years:\n",
    "        for season in seasons:\n",
    "            dir = os.path.join(os.getcwd(), f'data\\animes\\{year}\\{season}')\n",
    "            pathlib.Path(dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            print(f'{str(year)} {season}')\n",
    "            titles, hrefs = get_animes_by_season(year, season)\n",
    "            print(f'Animes: {len(titles)}')\n",
    "\n",
    "            for title, href in tqdm(zip(titles, hrefs)):\n",
    "                data = scrape_anime(title, href)\n",
    "                if data != np.nan:\n",
    "                    anime_id = str(data['Id'])\n",
    "                    data['Scrape_season'] = season\n",
    "                    data['Scrape_year'] = year\n",
    "                    data['Scrape_date'] = datetime.today().date().strftime('%Y-%m-%d')\n",
    "                    with open(f'{dir}\\{anime_id}.json', 'w') as f:\n",
    "                        json.dump(data, f)\n",
    "                \n",
    "                time.sleep(random.randint(2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 winter\n",
      "Animes: 261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [21:05,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 spring\n",
      "Animes: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [17:50,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 summer\n",
      "Animes: 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [22:03,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 fall\n",
      "Animes: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [23:49,  5.05s/it]\n"
     ]
    }
   ],
   "source": [
    "years = range(2020, 2021, 1)\n",
    "seasons = ['winter', 'spring', 'summer', 'fall']\n",
    "\n",
    "scrape_and_save_animes_by_season(years, seasons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
